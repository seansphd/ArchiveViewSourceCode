{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyARkzocdm9H052wdsZe2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seansphd/ArchiveViewSourceCode/blob/Update/ProcessPDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WP3TGxP55h9"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# PDF OCR and summarisation with batch processing for GitHub repositories\n",
        "# Output format: JSON\n",
        "\n",
        "# ======== INSTALL DEPENDENCIES ========\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package):\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "required_packages = [\n",
        "    \"pytesseract\",\n",
        "    \"pdf2image\",\n",
        "    \"transformers\",\n",
        "    \"torch\",\n",
        "    \"sentencepiece\",\n",
        "    \"einops\",\n",
        "    \"accelerate\",\n",
        "    \"spacy\",\n",
        "    \"textstat\",\n",
        "    \"tqdm\",\n",
        "    \"numpy\",\n",
        "    \"requests\",\n",
        "    \"PyPDF2\"\n",
        "]\n",
        "\n",
        "print(\"Checking and installing required packages...\")\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        install_package(package)\n",
        "\n",
        "# Install system dependencies if in Google Colab\n",
        "import os\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Installing system dependencies for Colab...\")\n",
        "    subprocess.run([\"apt-get\", \"update\"], check=True)\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"poppler-utils\", \"tesseract-ocr\", \"tesseract-ocr-eng\"], check=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "\n",
        "# ======== IMPORTS ========\n",
        "# Core libraries\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# PDF processing\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import PyPDF2  # For direct text extraction from PDFs\n",
        "\n",
        "# NLP and Text Analysis\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "# ML and Summarisation\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# GitHub API\n",
        "from urllib.parse import urljoin\n",
        "import urllib.parse\n",
        "\n",
        "# ======== SETUP ========\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load spaCy for NER and text analysis\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define available models\n",
        "available_models = {\n",
        "    \"BART CNN (fast)\": \"facebook/bart-large-cnn\",\n",
        "    \"T5 Small (fastest)\": \"t5-small\",\n",
        "    \"T5 Base (balanced)\": \"t5-base\",\n",
        "    \"FLAN-T5 Base (recommended)\": \"google/flan-t5-base\",\n",
        "    \"FLAN-T5 Large (better quality)\": \"google/flan-t5-large\"\n",
        "}\n",
        "\n",
        "# ======== GITHUB API FUNCTIONS ========\n",
        "def list_github_pdf_files(repo_owner, repo_name, path=\"\", token=None):\n",
        "    \"\"\"\n",
        "    List all PDF files in a GitHub repository recursively.\n",
        "\n",
        "    Args:\n",
        "        repo_owner (str): GitHub repository owner\n",
        "        repo_name (str): GitHub repository name\n",
        "        path (str): Path within the repository\n",
        "        token (str): GitHub personal access token\n",
        "\n",
        "    Returns:\n",
        "        list: List of PDF file dicts with name, path, and download_url\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
        "    }\n",
        "    if token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "\n",
        "    pdf_files = []\n",
        "\n",
        "    def fetch_contents(path_segment):\n",
        "        if path_segment:\n",
        "            url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{path_segment}\"\n",
        "        else:\n",
        "            url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents\"\n",
        "\n",
        "        print(f\"Accessing URL: {url}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error fetching contents: {response.status_code}\")\n",
        "                error_message = \"Unknown error\"\n",
        "                try:\n",
        "                    error_data = response.json()\n",
        "                    error_message = error_data.get('message', 'Unknown error')\n",
        "                except Exception:\n",
        "                    pass\n",
        "                print(error_message)\n",
        "\n",
        "                if response.status_code == 401 and token and \"Bearer\" in headers.get(\"Authorization\", \"\"):\n",
        "                    print(\"Trying alternative authorisation format...\")\n",
        "                    headers[\"Authorization\"] = f\"token {token}\"\n",
        "                    retry_response = requests.get(url, headers=headers, timeout=30)\n",
        "                    if retry_response.status_code == 200:\n",
        "                        print(\"Alternative authorisation successful\")\n",
        "                        response = retry_response\n",
        "                    else:\n",
        "                        print(f\"Alternative authorisation also failed: {retry_response.status_code}\")\n",
        "                        return\n",
        "                else:\n",
        "                    return\n",
        "\n",
        "            contents = response.json()\n",
        "\n",
        "            if isinstance(contents, list):\n",
        "                for item in contents:\n",
        "                    if item[\"type\"] == \"file\" and item[\"name\"].lower().endswith(\".pdf\"):\n",
        "                        pdf_files.append({\n",
        "                            \"name\": item[\"name\"],\n",
        "                            \"path\": item[\"path\"],\n",
        "                            \"download_url\": item[\"download_url\"]\n",
        "                        })\n",
        "                    elif item[\"type\"] == \"dir\":\n",
        "                        fetch_contents(item[\"path\"])\n",
        "            elif isinstance(contents, dict) and contents.get(\"type\") == \"file\" and contents[\"name\"].lower().endswith(\".pdf\"):\n",
        "                pdf_files.append({\n",
        "                    \"name\": contents[\"name\"],\n",
        "                    \"path\": contents[\"path\"],\n",
        "                    \"download_url\": contents[\"download_url\"]\n",
        "                })\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error: {e}\")\n",
        "\n",
        "    fetch_contents(path)\n",
        "    return pdf_files\n",
        "\n",
        "# ======== PDF PROCESSING FUNCTIONS ========\n",
        "def download_pdf(url, output_path):\n",
        "    \"\"\"Download a PDF file from a URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return output_path\n",
        "\n",
        "def ocr_pdf(pdf_path, dpi=300, preprocess=True, lang='eng'):\n",
        "    \"\"\"Extract text from PDF using OCR with optional image preprocessing.\"\"\"\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=dpi)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to images: {e}\")\n",
        "        return f\"Error processing PDF: {e}\", []\n",
        "\n",
        "    full_text = \"\"\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        if preprocess:\n",
        "            img_np = np.array(image)\n",
        "            img = Image.fromarray(img_np)\n",
        "            img = img.convert('L')  # greyscale\n",
        "            img = ImageEnhance.Contrast(img).enhance(2.0)\n",
        "            img = img.filter(ImageFilter.SHARPEN)\n",
        "            threshold = 150\n",
        "            img = img.point(lambda p: 255 if p > threshold else 0)\n",
        "            text = pytesseract.image_to_string(img, lang=lang, config='--psm 6')\n",
        "        else:\n",
        "            text = pytesseract.image_to_string(image, lang=lang)\n",
        "\n",
        "        full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
        "\n",
        "    return full_text, images\n",
        "\n",
        "def extract_text_directly(pdf_path):\n",
        "    \"\"\"Extract text directly from PDF without OCR.\"\"\"\n",
        "    try:\n",
        "        full_text = \"\"\n",
        "        num_pages = 0\n",
        "\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            for i, page in enumerate(pdf_reader.pages):\n",
        "                text = page.extract_text() or \"\"\n",
        "                full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
        "\n",
        "        images = [None] * num_pages\n",
        "        return full_text, images\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text directly from PDF: {e}\")\n",
        "        return f\"Error processing PDF: {e}\", []\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Improve text quality after extraction.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.replace('|', 'I')\n",
        "    # Avoid replacing zero with O to keep numbers intact\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?\\'\"-]', '', text)\n",
        "    text = re.sub(r'([.,;:!?])(\\w)', r'\\1 \\2', text)\n",
        "    return text\n",
        "\n",
        "# ======== TEXT ANALYSIS FUNCTIONS ========\n",
        "def summarise_text(text, model_name=\"facebook/bart-large-cnn\", chunk_size=1024, max_length=150, min_length=50):\n",
        "    \"\"\"Summarise text with a Hugging Face model.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text to summarise.\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "        summariser = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
        "\n",
        "        prefix = \"summarize: \" if \"t5\" in model_name else \"\"\n",
        "\n",
        "        # Sentence aware chunking using spaCy\n",
        "        doc = nlp(text)\n",
        "        sentences = [s.text for s in doc.sents]\n",
        "\n",
        "        chunks = []\n",
        "        buf = \"\"\n",
        "        for s in sentences:\n",
        "            if len(tokenizer.encode(buf + s)) < chunk_size:\n",
        "                buf += s + \" \"\n",
        "            else:\n",
        "                chunks.append(buf.strip())\n",
        "                buf = s + \" \"\n",
        "        if buf.strip():\n",
        "            chunks.append(buf.strip())\n",
        "\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "            input_text = prefix + chunk\n",
        "            try:\n",
        "                out = summariser(input_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "                summaries.append(out[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarising chunk: {e}\")\n",
        "                continue\n",
        "\n",
        "        combined = \" \".join(summaries).strip()\n",
        "\n",
        "        if not combined:\n",
        "            combined = text[:800]\n",
        "\n",
        "        if len(tokenizer.encode(combined)) > chunk_size:\n",
        "            combined = summariser(prefix + combined, max_length=max_length*2, min_length=min_length, do_sample=False)[0]['summary_text']\n",
        "\n",
        "        return combined\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarisation: {e}\")\n",
        "        return f\"Error in summarisation: {e}\"\n",
        "\n",
        "def extract_keywords(text, top_n=10):\n",
        "    \"\"\"Extract keywords from text.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text[:500000])  # safety limit\n",
        "        keywords = []\n",
        "\n",
        "        for chunk in doc.noun_chunks:\n",
        "            keywords.append(chunk.text)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            keywords.append(ent.text)\n",
        "\n",
        "        keyword_counter = Counter(keywords)\n",
        "\n",
        "        filtered = {}\n",
        "        for k, v in keyword_counter.items():\n",
        "            tk = nlp(k)\n",
        "            if len(k) > 3 and not all(t.is_stop for t in tk):\n",
        "                filtered[k] = v\n",
        "\n",
        "        top_keywords = dict(sorted(filtered.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
        "        return top_keywords\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def analyse_text(text):\n",
        "    \"\"\"Compute text statistics.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        stats = {\n",
        "            'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
        "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
        "            'smog_index': textstat.smog_index(text),\n",
        "            'coleman_liau_index': textstat.coleman_liau_index(text),\n",
        "            'automated_readability_index': textstat.automated_readability_index(text),\n",
        "            'dale_chall_readability_score': textstat.dale_chall_readability_score(text),\n",
        "            'word_count': textstat.lexicon_count(text),\n",
        "            'sentence_count': textstat.sentence_count(text),\n",
        "            'avg_sentence_length': textstat.avg_sentence_length(text),\n",
        "            'avg_syllables_per_word': textstat.avg_syllables_per_word(text)\n",
        "        }\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        print(f\"Error analysing text: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"Named Entity Recognition.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text[:500000])  # safety limit\n",
        "        entities = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ not in entities:\n",
        "                entities[ent.label_] = []\n",
        "            if ent.text not in entities[ent.label_]:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "\n",
        "        return entities\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting entities: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# ======== MAIN PROCESSING FUNCTION ========\n",
        "def process_pdf(url, pdf_name, model_name=\"facebook/bart-large-cnn\", dpi=300, preprocess=True, use_ocr=True):\n",
        "    \"\"\"Process a single PDF file.\"\"\"\n",
        "    results = {\n",
        "        \"pdf_name\": pdf_name,\n",
        "        \"url\": url,\n",
        "        \"status\": \"success\",\n",
        "        \"error\": None,\n",
        "        \"num_pages\": 0,\n",
        "        \"word_count\": 0,\n",
        "        \"summary\": \"\",\n",
        "        \"top_keywords\": {},\n",
        "        \"readability\": {},\n",
        "        \"entities\": {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            pdf_path = os.path.join(temp_dir, \"document.pdf\")\n",
        "            download_pdf(url, pdf_path)\n",
        "\n",
        "            if use_ocr:\n",
        "                print(f\"Performing OCR on {pdf_name}...\")\n",
        "                raw_text, images = ocr_pdf(pdf_path, dpi=dpi, preprocess=preprocess)\n",
        "            else:\n",
        "                print(f\"Extracting text directly from {pdf_name}...\")\n",
        "                raw_text, images = extract_text_directly(pdf_path)\n",
        "\n",
        "            results[\"num_pages\"] = len(images) if images else 0\n",
        "\n",
        "            text = clean_text(raw_text)\n",
        "            results[\"word_count\"] = len(text.split())\n",
        "\n",
        "            results[\"summary\"] = summarise_text(text, model_name=model_name)\n",
        "\n",
        "            keywords = extract_keywords(text)\n",
        "            results[\"top_keywords\"] = keywords\n",
        "\n",
        "            text_stats = analyse_text(text)\n",
        "            results[\"readability\"] = text_stats\n",
        "\n",
        "            entities = extract_entities(text)\n",
        "            results[\"entities\"] = entities\n",
        "\n",
        "    except Exception as e:\n",
        "        results[\"status\"] = \"failed\"\n",
        "        results[\"error\"] = str(e)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ======== JSON SAVE FUNCTION ========\n",
        "def save_results_to_json(results, output_file):\n",
        "    \"\"\"Save processing results to a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results to JSON: {e}\")\n",
        "\n",
        "# ======== BATCH PROCESSING FUNCTION ========\n",
        "def batch_process_pdfs(pdf_files, model_name=\"facebook/bart-large-cnn\", dpi=300,\n",
        "                       preprocess=True, use_ocr=True, output_json=\"pdf_summaries.json\"):\n",
        "    \"\"\"Process multiple PDF files and save results to JSON.\"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for i, pdf_file in enumerate(tqdm(pdf_files, desc=\"Processing PDFs\")):\n",
        "        print(f\"\\n[{i+1}/{len(pdf_files)}] Processing: {pdf_file['name']}\")\n",
        "        result = process_pdf(\n",
        "            pdf_file[\"download_url\"],\n",
        "            pdf_file[\"name\"],\n",
        "            model_name=model_name,\n",
        "            dpi=dpi,\n",
        "            preprocess=preprocess,\n",
        "            use_ocr=use_ocr\n",
        "        )\n",
        "        all_results.append(result)\n",
        "\n",
        "        # Save progress after each file\n",
        "        save_results_to_json(all_results, output_json)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ======== MAIN FUNCTION ========\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    # Configuration\n",
        "    repo_owner = input(\"Enter GitHub repository owner: \")\n",
        "    repo_name = input(\"Enter GitHub repository name: \")\n",
        "    path = input(\"Enter repository path (leave blank for root): \")\n",
        "    token = input(\"Enter GitHub token (leave blank if not needed): \")\n",
        "\n",
        "    if not token:\n",
        "        token = None\n",
        "\n",
        "    use_ocr = input(\"\\nDo the PDFs require OCR? (y/n): \").lower() == 'y'\n",
        "\n",
        "    dpi = 300\n",
        "    preprocess = True\n",
        "    if use_ocr:\n",
        "        try:\n",
        "            dpi = int(input(\"Enter OCR resolution (DPI, recommended 300): \") or \"300\")\n",
        "        except ValueError:\n",
        "            dpi = 300\n",
        "        preprocess = input(\"Enhance images before OCR? (y/n): \").lower() == 'y'\n",
        "    else:\n",
        "        print(\"Skipping OCR and using direct text extraction...\")\n",
        "\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, (name, _) in enumerate(available_models.items()):\n",
        "        print(f\"{i+1}. {name}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            model_idx = int(input(\"\\nSelect model (1-5): \")) - 1\n",
        "            if 0 <= model_idx < len(available_models):\n",
        "                break\n",
        "        except ValueError:\n",
        "            pass\n",
        "        print(\"Please enter a number between 1 and 5.\")\n",
        "    model_name = list(available_models.values())[model_idx]\n",
        "\n",
        "    output_json = input(\"Enter output JSON filename (default: pdf_summaries.json): \") or \"pdf_summaries.json\"\n",
        "\n",
        "    print(\"\\nFetching PDF files from repository...\")\n",
        "    pdf_files = list_github_pdf_files(repo_owner, repo_name, path, token)\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files found in the repository.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(pdf_files)} PDF files:\")\n",
        "    for i, pdf in enumerate(pdf_files):\n",
        "        print(f\"{i+1}. {pdf['path']}\")\n",
        "\n",
        "    confirm = input(f\"\\nProcess {len(pdf_files)} PDFs? (y/n): \").lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"Operation cancelled.\")\n",
        "        return\n",
        "\n",
        "    results = batch_process_pdfs(\n",
        "        pdf_files,\n",
        "        model_name=model_name,\n",
        "        dpi=dpi,\n",
        "        preprocess=preprocess,\n",
        "        use_ocr=use_ocr,\n",
        "        output_json=output_json\n",
        "    )\n",
        "\n",
        "    print(f\"\\nProcessing complete. Results saved to {output_json}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#example prompt answers\n",
        "#1 seansphd, 2 PAGE-Archive - leave the rest blank"
      ]
    }
  ]
}